{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiiGXZDrWk3J",
        "colab_type": "code",
        "outputId": "947ea0f3-e8e7-4ec8-ea4f-1e307bf38706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZAaY--Sc5Gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "import re\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import math\n",
        "import codecs\n",
        "import random\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from string import punctuation\n",
        "from nltk.tokenize import sent_tokenize  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQrhiAkQc5Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Text_ = open(\"/content/Trump_Speech.txt\",\"r\")\n",
        "text = Text_.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTI3SVZOUBil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.replace('SPEECH','')\n",
        "text = text.replace('\\n','')\n",
        "text = text.replace('\\'', '')\n",
        "text = re.sub('[0-9]', r'', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mupY9CobUHk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_sent = sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FCJMLEgbviR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "Final_list = []\n",
        "text_ = []\n",
        "d = 0\n",
        "exclude = set(string.punctuation)\n",
        "for i in list_sent:\n",
        "  s = ''.join(ch for ch in i if ch not in exclude)\n",
        "  s = s.lower()\n",
        "  d = d + len(s.split())\n",
        "  text_ = text_ + [s]\n",
        "  s = '<s> '+ s + ' </s>'\n",
        "  Final_list = Final_list + [s]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWeT2HBrx1EA",
        "colab_type": "code",
        "outputId": "f1e72d78-1a54-4105-e489-db00b720263f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(f\"Total Number of Tokens {d}\")\n",
        "print('Number of Sentences are ' + str(len(Final_list)))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Tokens 162723\n",
            "Number of Sentences are 12714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyHqsbnmjmFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(Final_list, test_size = 0.20, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_cA_AyxazH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_ngrams(data, num):\n",
        "  n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "  return [' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d942KjB8jsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = {}\n",
        "for sent in Final_list:\n",
        "  sent = sent.split()\n",
        "  for i in sent:\n",
        "    if i in vocab:\n",
        "      vocab[i] += 1\n",
        "    else:\n",
        "      vocab[i] = 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG0iR-ms_u0R",
        "colab_type": "text"
      },
      "source": [
        "#Computing MLE of Unigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydFJ7Knq3S96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b849483c-9289-45bd-b00e-86d3d0e32bba"
      },
      "source": [
        "total_token = sum(vocab.values())\n",
        "k = 0\n",
        "for i in vocab:\n",
        "  k = k + 1\n",
        "  print(i, vocab[i]/total_token)\n",
        "  \n",
        "  if k == 5:\n",
        "    break  "
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> 0.06757338520656281\n",
            "thank 0.0010417164936673204\n",
            "you 0.014940127876014477\n",
            "so 0.006260928722143385\n",
            "much 0.0016104086611285616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H09SdXzoA-Ct",
        "colab_type": "text"
      },
      "source": [
        "#Computing MLE of Bigrams, Trigrams and Quadgrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CP5WZ8zKLZE",
        "colab_type": "text"
      },
      "source": [
        "1. For **Bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HvE088m_eDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram(list_,n):  \n",
        "  gram = {}\n",
        "  for sent in list_:\n",
        "    sent = sent.split()\n",
        "    for i in range(1,len(sent) - n ):\n",
        "      word_pair = (sent[i],sent[i+1])\n",
        " \n",
        "      if word_pair in gram:\n",
        "        gram[word_pair] += 1\n",
        "      else:\n",
        "        gram[word_pair] = 1    \n",
        "  return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIawIWKXCTD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f8eaf5c1-7b0f-4176-86b6-5f1680ff4943"
      },
      "source": [
        "MLE_bigram = {}\n",
        "MLE_bg = {}\n",
        "for i in [2]:\n",
        "  gram = ngram(list_ = Final_list, n = i)\n",
        "  p = 0\n",
        "  for j in gram:\n",
        "    l = gram[j]\n",
        "    m = vocab[j[0]]\n",
        "    MLE_bigram[j] = l/m\n",
        "    MLE_bg[j] = l \n",
        "p = 0     \n",
        "for i in MLE_bigram:\n",
        "  print('The MLE for word pair {' + str(i) + '} is ' + str(MLE_bigram[i]))\n",
        "  p = p + 1\n",
        "  if p == 5:\n",
        "    break\n"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MLE for word pair {('thank', 'you')} is 0.6785714285714286\n",
            "The MLE for word pair {('you', 'so')} is 0.0024902170046246885\n",
            "The MLE for word pair {('so', 'much')} is 0.07555178268251274\n",
            "The MLE for word pair {('thats', 'so')} is 0.04\n",
            "The MLE for word pair {('so', 'nice')} is 0.00933786078098472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW46MTJGKRiq",
        "colab_type": "text"
      },
      "source": [
        "2. For **Trigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad5QqFaaLkY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram(list_,n):  \n",
        "  gram = {}\n",
        "  for sent in list_:\n",
        "    sent = sent.split()\n",
        "    for i in range(1,len(sent) - n ):\n",
        "      word_pair = (sent[i],sent[i+1],sent[i+2])\n",
        " \n",
        "      if word_pair in gram:\n",
        "        gram[word_pair] += 1\n",
        "      else:\n",
        "        gram[word_pair] = 1    \n",
        "  return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VOqxXk5Gv3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0389fb56-dc68-4b9c-bf66-439c57a6b32b"
      },
      "source": [
        "MLE_Trigram = {}\n",
        "MLE_Tg = {}\n",
        "for i in [3]:\n",
        "  gram = ngram(list_ = Final_list, n = i)\n",
        "  p = 0\n",
        "  for j in gram:\n",
        "    l = gram[j]\n",
        "    m = MLE_bg[j[0],j[1]]\n",
        "    MLE_Trigram[j] = l/m\n",
        "    MLE_Tg[j] = l\n",
        "    \n",
        "p = 0     \n",
        "for i in MLE_Trigram:\n",
        "  print('The MLE for word pair {' + str(i) + '} is ' + str(MLE_Trigram[i]))\n",
        "  p = p + 1\n",
        "  if p == 5:\n",
        "    break"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MLE for word pair {('thank', 'you', 'so')} is 0.015037593984962405\n",
            "The MLE for word pair {('you', 'so', 'much')} is 0.2857142857142857\n",
            "The MLE for word pair {('thats', 'so', 'nice')} is 0.5\n",
            "The MLE for word pair {('isnt', 'he', 'a')} is 1.0\n",
            "The MLE for word pair {('he', 'a', 'great')} is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhZayB5SMSUH",
        "colab_type": "text"
      },
      "source": [
        "3. For **Quadgrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99PZq7jKdYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram(list_,n):  \n",
        "  gram = {}\n",
        "  for sent in list_:\n",
        "    sent = sent.split()\n",
        "    for i in range(1,len(sent) - n ):\n",
        "      word_pair = (sent[i],sent[i+1],sent[i+2],sent[i+3])\n",
        "      \n",
        "      if word_pair in gram:\n",
        "        gram[word_pair] += 1\n",
        "      else:\n",
        "        gram[word_pair] = 1    \n",
        "  return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh_AYuenMd7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "528a2b4c-733b-472a-a73e-f5073e1dc0f5"
      },
      "source": [
        "MLE_quadgram = {}\n",
        "MLE_qg = {}\n",
        "for i in [3]:\n",
        "  gram = ngram(list_ = Final_list, n = i)\n",
        "  p = 0\n",
        "  for j in gram:\n",
        "    l = gram[j]\n",
        "    m = MLE_Tg[j[0],j[1],j[2]]\n",
        "    MLE_quadgram[j] = l/m\n",
        "    MLE_qg[j] = l\n",
        "    \n",
        "p = 0     \n",
        "for i in MLE_quadgram:\n",
        "  print('The MLE for word pair {' + str(i) + '} is ' + str(MLE_dendgram[i]))\n",
        "  p = p + 1\n",
        "  if p == 5:\n",
        "    break"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MLE for word pair {('thank', 'you', 'so', 'much')} is 1.0\n",
            "The MLE for word pair {('you', 'so', 'much', '</s>')} is 1.0\n",
            "The MLE for word pair {('thats', 'so', 'nice', '</s>')} is 1.0\n",
            "The MLE for word pair {('isnt', 'he', 'a', 'great')} is 1.0\n",
            "The MLE for word pair {('he', 'a', 'great', 'guy')} is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0RsXutObSmH",
        "colab_type": "text"
      },
      "source": [
        "#Generating sentences using n-gram!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q977mGLbd7nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MLE_All = {1:vocab , 2: MLE_bigram, 3 : MLE_Trigram, 4:MLE_quadgram}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Kx8I7-hxOqA",
        "colab": {}
      },
      "source": [
        "      def generating(n):\n",
        "    \n",
        "        dict_ngram = {}\n",
        "        dict_ngrams = {}\n",
        "        for sent in Final_list:\n",
        "            list_ngram = list(ngrams(sent.split(), n))\n",
        "            list_ngrams = list(ngrams(sent.split(), n-1))\n",
        "            for i in list_ngram:\n",
        "                if i in dict_ngram:\n",
        "                    dict_ngram[i] += 1\n",
        "                else:\n",
        "                    dict_ngram[i] = 1 \n",
        "            for i in list_ngrams:\n",
        "                if i in dict_ngram:\n",
        "                    dict_ngrams[i] += 1\n",
        "                else:\n",
        "                    dict_ngrams[i] = 1 \n",
        "        \n",
        "        #Till here we have created a dictionary of n & (n - 1) grams now lets calculat the MLE values for it            \n",
        "        #calculating MLE for ngram model\n",
        "        \n",
        "        MLE_ngram = {}\n",
        "        for i in dict_ngram.keys():\n",
        "            li = []\n",
        "            for j in range(n-1):\n",
        "                li.append(i[j])\n",
        "            MLE_ngram[i] = (dict_ngram[i]*1.0)/dict_ngrams[tuple(li)]\n",
        "        s = []\n",
        "        gen_ngram = []\n",
        "        gen_prob = []\n",
        "        for i in dict_ngram.keys():\n",
        "            if i[0] == '<s>':\n",
        "                gen_ngram.append(i)\n",
        "                gen_prob.append(MLE_ngram[i])\n",
        "        norm_prob = np.divide(gen_prob, sum(gen_prob))\n",
        "        a = np.random.multinomial(1, norm_prob, size=1).tolist()\n",
        "        index = a[0].index(1)\n",
        "        for i in gen_ngram[index]:\n",
        "            s.append(i)\n",
        "        while(s[-1] != '</s>' and len(s)<25):\n",
        "            gen_ngram = []\n",
        "            gen_prob = []\n",
        "            for b in dict_ngram.keys(): \n",
        "                if(list(b[:-1]) == s[-n+1:]):\n",
        "                    gen_ngram.append(b)\n",
        "                    gen_prob.append(MLE_ngram[b])\n",
        "            norm_prob = np.divide(gen_prob, sum(gen_prob))\n",
        "            a = np.random.multinomial(1, norm_prob, size=1).tolist()\n",
        "            index = a[0].index(1)\n",
        "            s.append(gen_ngram[index][-1])\n",
        "        print(' '.join(s))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojj6ZGbTn6ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3ba2689a-3e6b-40c9-e3ce-5d19abe8bfe0"
      },
      "source": [
        "for i in range(5):\n",
        "  generating(n = 3)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> there are many of these people about trump </s>\n",
            "<s> he spent — think of it </s>\n",
            "<s> if you or if you’ve been great friends from japan from so many waysin recent years i’ve been hearing that his daughter works for\n",
            "<s> they’ve never seen anything like it is possible somebody slipped — hey we do business with you </s>\n",
            "<s> but i would have said no they’re so into it because the border </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2rkOS8L1C9g",
        "colab_type": "text"
      },
      "source": [
        "The text above seems OK. Although there are some faults with the meaning of the sentences but gramatically they are correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJI02s41o5g",
        "colab_type": "text"
      },
      "source": [
        "#Neural Approach on the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQKCvS1KgC4",
        "colab_type": "text"
      },
      "source": [
        "#Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94TOJTNG6I8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout,SimpleRNN \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as utils\n",
        "import numpy as np\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLrNdb8hyEKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(text_, test_size = 0.20, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3LW1xL8Am0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "def sequence_of_tokens(text):\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    #converting data to a sequence of tokens \n",
        "    input_ = []\n",
        "    for line in text:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_.append(n_gram_sequence)\n",
        "    return input_\n",
        "\n",
        "inp_  = sequence_of_tokens(text = train)\n",
        "max_seq_len = max([len(x) for x in inp_])\n",
        "inp_ = np.array(pad_sequences(inp_, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "predictors, label = inp_[:,:-1],inp_[:,-1]\n",
        "label = utils.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYcMaxV1Dc6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_generation(seed_text, next_words, max_sequence_len, model):\n",
        "  for j in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen= max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict_classes(token_list, verbose=0)\n",
        "  \n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += \" \" + output_word\n",
        "  return seed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E340_cb8Dc1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "9806118a-672b-401b-e2e2-3cb4a99cdc46"
      },
      "source": [
        "input_len = max_seq_len-1\n",
        "model=Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=input_len))\n",
        "model.add(SimpleRNN(200))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpointer = ModelCheckpoint(verbose=1, save_best_only=True)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(predictors, label, batch_size=128, epochs = 5,callbacks = callbacks_list)"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 308, 100)          792200    \n",
            "_________________________________________________________________\n",
            "simple_rnn_10 (SimpleRNN)    (None, 200)               60200     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 7922)              1592322   \n",
            "=================================================================\n",
            "Total params: 2,444,722\n",
            "Trainable params: 2,444,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "119643/119643 [==============================] - 270s 2ms/step - loss: 6.9068\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.90683, saving model to weights-improvement-01-6.9068.hdf5\n",
            "Epoch 2/5\n",
            "119643/119643 [==============================] - 272s 2ms/step - loss: 6.1162\n",
            "\n",
            "Epoch 00002: loss improved from 6.90683 to 6.11623, saving model to weights-improvement-02-6.1162.hdf5\n",
            "Epoch 3/5\n",
            "119643/119643 [==============================] - 274s 2ms/step - loss: 5.7468\n",
            "\n",
            "Epoch 00003: loss improved from 6.11623 to 5.74680, saving model to weights-improvement-03-5.7468.hdf5\n",
            "Epoch 4/5\n",
            "119643/119643 [==============================] - 273s 2ms/step - loss: 5.5322\n",
            "\n",
            "Epoch 00004: loss improved from 5.74680 to 5.53216, saving model to weights-improvement-04-5.5322.hdf5\n",
            "Epoch 5/5\n",
            "119643/119643 [==============================] - 273s 2ms/step - loss: 5.7600\n",
            "\n",
            "Epoch 00005: loss did not improve from 5.53216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f50e8907cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S6fnAA_AmyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "ad281611-04ca-4a4e-9a83-8631989f2aeb"
      },
      "source": [
        "seed_list = ['Who are', 'I am the', 'This world', 'America wants ', 'The economy is']\n",
        "print('Generated text given seed text')\n",
        "for i in seed_list:\n",
        "  print(text_generation(i,8,309,model))"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated text given seed text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-300-5cf8f7796ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated text given seed text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m309\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-289-155720d54729>\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(seed_text, next_words, max_sequence_len, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_13_input to have shape (14,) but got array with shape (308,)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhdPO4BCLJUq",
        "colab_type": "text"
      },
      "source": [
        "Calculating the **Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpgYIGKvLPw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying on the Test_dataset\n",
        "output_seq = sequence_of_tokens(text = test)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in output_seq])\n",
        "output_seq= np.array(pad_sequences(output_seq,   \n",
        "                          maxlen=15, padding='pre'))\n",
        "\n",
        "a, b = output_sequences[:,:-1],output_sequences[:,-1]\n",
        "b = utils.to_categorical(b, num_classes=total_words)\n",
        "score = model.evaluate(a,b, verbose=False)\n",
        "print(\"The perplexity of the model on test set is \", np.exp(score[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyau5qYOKYh4",
        "colab_type": "text"
      },
      "source": [
        "#LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd0tEzQjKbkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "557a952b-18bc-4285-a3b0-beeeac338546"
      },
      "source": [
        "input_len = max_seq_len-1\n",
        "model=Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=input_len))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights_rnn.hdf5', verbose=1, save_best_only=True)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(predictors, label, batch_size= 512, epochs = 5,callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 308, 100)          792200    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 200)               240800    \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 7922)              1592322   \n",
            "=================================================================\n",
            "Total params: 2,625,322\n",
            "Trainable params: 2,625,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "119643/119643 [==============================] - 204s 2ms/step - loss: 6.6897\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.68968, saving model to weights-improvement-01-6.6897.hdf5\n",
            "Epoch 2/5\n",
            " 18432/119643 [===>..........................] - ETA: 2:49 - loss: 6.4331"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJv-m_5KtVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_list = ['Who are', 'I am the', 'This world', 'America wants ', 'The economy is']\n",
        "\n",
        "print('Generated text given seed text')\n",
        "for i in seed_list:\n",
        "  print(text_generation(i,np.random.randint(5,10),10,model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTA1lzfWN61p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying on the Test_dataset & using the function defined above\n",
        "output_seq = sequence_of_tokens(text = test)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in output_seq])\n",
        "output_seq= np.array(pad_sequences(output_seq,   \n",
        "                          maxlen=15, padding='pre'))\n",
        "\n",
        "a, b = output_sequences[:,:-1],output_sequences[:,-1]\n",
        "b = utils.to_categorical(b, num_classes=total_words)\n",
        "score = model.evaluate(a,b, verbose=False)\n",
        "print(\"The perplexity of the model on test set is \", np.exp(score[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bobPCFbOtbJ",
        "colab_type": "text"
      },
      "source": [
        " **Looking at the perplexities of LSTM and RNN models we can clearly say that LSTM has performed better than RNN. \n",
        "Also when comparing between Neural network based approaches and Classical approaches we can say that neural network generalises better. The classic approaches (n-gram approach) only take into account the previous (n-1) words which have occured but neural network can create sentences which it has not even seen. Thus we can say that neural networks has a better tendancy of generalising and would perform better than Classical approaches when given larger datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMa-M56IOVXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}